---
title: "Mathematical Foundations"
description: "Provides notes on essential mathematical concepts like linear algebra."
sidebar_position: 6
---
# Mathematical Foundations

This section provides a foundational understanding of the mathematical concepts essential for grasping the mechanics behind Vision Transformers. We'll delve into linear algebra, covering vectors, matrices, transformations, and related concepts, drawing from various notes to build a comprehensive picture.

## 1. Vectors

Vectors are fundamental building blocks in linear algebra, representing quantities with both magnitude and direction.

*   **Perspectives on Vectors:**
    *   **Physics:** An arrow with a specific length and direction.
    *   **Computer Science:** An ordered list of numbers.
    *   **Mathematics:** An entity on which vector addition and scalar multiplication can be performed.

*   **Representation in Linear Algebra:**
    *   In 2D space, a vector is represented by a 2x1 column matrix, where elements denote the x and y coordinates.
    *   In 3D space, a vector is represented by a 3x1 matrix, with elements denoting x, y, and z coordinates.

*   **Basic Vector Operations:**
    *   **Vector Addition:** Geometrically, this involves placing the tail of the second vector at the head of the first, with the resultant vector spanning from the tail of the first to the head of the second. Algebraically, it's the element-wise sum of the vectors.
    *   **Scalar Multiplication:** Multiplying a vector by a scalar scales its length. If the scalar is negative, the vector's direction is also reversed.

```python
# Example of scalar multiplication
vector = [2, 5]
scalar = 3
scaled_vector = [scalar * component for component in vector]
# scaled_vector will be [6, 15]
```

## 2. Basis Vectors, Linear Combination, and Span

These concepts define how vectors can generate and describe spaces.

*   **Basis Vectors:** A set of linearly independent vectors that can span the entire vector space. In a 2D plane, the standard basis vectors are **i hat** ([1, 0]) and **j hat** ([0, 1]).

*   **Linear Combination:** The sum of scaled vectors. For vectors **v** and **w** and scalars 'a' and 'b', a linear combination is `a*v + b*w`.

*   **Span:** The set of all possible vectors that can be reached by forming linear combinations of a given set of vectors. For example, the span of **i hat** and **j hat** in 2D space is the entire 2D plane.

*   **Linearly Dependent Vectors:** A set of vectors where one vector can be expressed as a linear combination of the others. This means that vector does not add any new dimensions to the span.

## 3. Linear Transformations and Matrices

Linear transformations are functions that map vectors to other vectors while preserving certain properties, and matrices are a powerful tool to represent these transformations.

*   **Definition:** A transformation is linear if it preserves vector addition and scalar multiplication. Geometrically, this means that lines remain lines, and the origin remains fixed.

*   **Matrix Representation:** In 2D space, a linear transformation can be represented by a 2x2 matrix. The columns of this matrix show where the original basis vectors (**i hat** and **j hat**) land after the transformation.

*   **Applying Transformations:** To transform an arbitrary vector, you multiply its coordinate matrix by the transformation matrix.

```python
# Matrix multiplication for transformation
transformation_matrix = [[a, b], [c, d]]
vector_coords = [[x], [y]]
transformed_coords = [[a*x + b*y], [c*x + d*y]]
```

*   **Matrices as Transformations:** Matrices are not just arrays of numbers; they embody the geometric operations of scaling, rotation, shearing, and more.

## 4. Matrix Multiplication as Composition

When multiple linear transformations are applied sequentially, their effect can be combined into a single transformation matrix.

*   **Composition:** Applying one transformation followed by another.
*   **Matrix Multiplication:** The product of two matrices (e.g., AB) represents the composition of the transformations they embody. The order of multiplication matters (AB is generally not equal to BA), reflecting the order in which the transformations are applied (typically, the right matrix is applied first).

## 5. Three-Dimensional Linear Transformations

The principles of linear transformations extend to three dimensions. Transformations are represented by 3x3 matrices, and the underlying concepts of vectors, linear combinations, and span apply similarly.

## 6. The Determinant

The determinant of a square matrix quantifies how a linear transformation scales areas (in 2D) or volumes (in 3D).

*   **Geometric Interpretation:**
    *   **2D:** The determinant is the factor by which the area of a unit square (formed by basis vectors) changes after the transformation. A negative determinant indicates a flip in orientation.
    *   **3D:** The determinant represents the scaling factor for volume. A determinant of zero signifies that the space has been collapsed into a lower dimension (plane, line, or point).

```python
# Determinant of a 2x2 matrix [[a, b], [c, d]]
determinant = a*d - b*c
```

## 7. Inverse Matrices, Column Space, and Null Space

These concepts are crucial for understanding how to reverse transformations and analyze the output of a transformation.

*   **Inverse Matrix (A⁻¹):** If a matrix 'A' represents a transformation, its inverse 'A⁻¹' represents the transformation that undoes 'A'. An inverse exists only if the determinant of 'A' is non-zero. Multiplying a matrix by its inverse results in the identity matrix (representing no change).

*   **Rank:** The dimension of the output space of a linear transformation. It indicates the number of linearly independent output vectors.

*   **Column Space:** The set of all possible output vectors that can be generated by a transformation. The rank of a matrix is the dimension of its column space.

*   **Null Space (Kernel):** The set of all input vectors that are mapped to the zero vector by a transformation. If a matrix doesn't have full rank, its null space will contain more than just the zero vector.

## 8. Non-Square Matrices as Transformations Between Dimensions

Non-square matrices represent transformations that change the dimensionality of the input space.

*   A 3x2 matrix transforms a 2D input into a 3D output, typically mapping the 2D space onto a plane within the 3D space.
*   A 2x3 matrix transforms a 3D input into a 2D output.

## 9. Dot Product and Duality

The dot product has significant geometric and algebraic interpretations.

*   **Dot Product:** For two vectors **v** and **w**, the dot product (**v** ⋅ **w**) is the product of the length of one vector and the length of the projection of the other vector onto the first. It's also the sum of the products of corresponding components.

*   **Duality:** There's a correspondence between vectors and linear transformations that map a space to a 1D space (a number line). Applying such a transformation is equivalent to taking the dot product with a specific vector.

```python
# Dot product of two vectors v and w
dot_product = sum(v_i * w_i for v_i, w_i in zip(v, w))
```

## 10. Cross Products

Primarily used in 3D space, the cross product yields a vector perpendicular to two input vectors.

*   **Magnitude:** The magnitude of the cross product vector is equal to the area of the parallelogram formed by the two input vectors.
*   **Direction:** Determined by the right-hand rule.
*   **Relation to Determinants:** The cross product of two 2D vectors can be related to the determinant of a matrix formed by those vectors.

## 11. Cross Products in the Light of Linear Transformations

The cross product can be understood through the lens of linear transformations, particularly in relation to duality and determinants. The geometric interpretation connects the volume of a parallelepiped to the dot product and cross product.

## 12. Cramer's Rule - Geometric Interpretation

Cramer's rule provides a method for solving systems of linear equations using determinants. Geometrically, it relates the solution components to scaled areas (in 2D) or volumes (in 3D) formed by replacing columns of the coefficient matrix with the result vector.

## 13. Change of Basis

This concept allows us to represent vectors and transformations in different coordinate systems (bases).

*   **Translating Coordinates:** A "change of basis matrix" can transform coordinates from one basis to another. The inverse of this matrix allows translation in the opposite direction.
*   **Transforming Matrices:** To apply a transformation in a new basis, you typically:
    1.  Translate the vector to the original basis.
    2.  Apply the original transformation.
    3.  Translate the result back to the new basis using the inverse change of basis matrix.
    *   The relationship is often expressed as: `New_Transformation = A⁻¹ * Original_Transformation * A`, where 'A' is the change of basis matrix.

## 14. Eigenvectors and Eigenvalues

Eigenvectors are special vectors that, when a linear transformation is applied, only get scaled (stretched or compressed) and do not change their direction (they remain on their original span). The scaling factor is the eigenvalue.

*   **Equation:** `A * v = λ * v`, where 'A' is the transformation matrix, 'v' is the eigenvector, and 'λ' (lambda) is the eigenvalue.
*   **Significance:** Eigenvectors and eigenvalues reveal fundamental properties of a transformation, such as axes of rotation or directions of maximum stretch. For instance, in a 3D rotation, the axis of rotation is an eigenvector with an eigenvalue of 1.
*   **Calculation:** This involves solving the characteristic equation: `det(A - λI) = 0`, where 'I' is the identity matrix.

## 15. A Quick Trick for Computing Eigenvalues

The characteristic polynomial `det(A - λI)` is used to find the eigenvalues. The roots of this polynomial are the eigenvalues of the matrix 'A'.

## 16. Abstract Vector Spaces

Linear algebra concepts can be generalized beyond simple arrows or lists of numbers.

*   **Functions as Vectors:** Functions can be treated as vectors. They can be added (pointwise) and scaled.
*   **Linear Operators:** Transformations applied to functions are called linear operators (e.g., differentiation).
*   **Basis Functions:** Just as **i hat**, **j hat**, **k hat** form a basis for 3D space, specific functions (like polynomials `1, x, x^2, ...`) can form a basis for function spaces.
*   **Matrix Representation of Operators:** A linear operator like differentiation can be represented by a matrix within a chosen basis. Multiplying this matrix by a vector representing a function yields a vector representing the derivative of that function.
*   **Vector Space Axioms:** Abstract vector spaces are defined by a set of axioms that govern vector addition and scalar multiplication, ensuring consistency across different mathematical objects (like arrows, matrices, or functions) that behave like vectors.

```python
# Example of representing polynomial basis functions
# Basis: [1, x, x^2]
# A polynomial like 2 + 3x + 4x^2 can be represented as [2, 3, 4]
```

## Key Takeaways

*   Linear algebra provides the mathematical language for understanding transformations in space.
*   Vectors, matrices, and linear transformations are interconnected concepts.
*   The determinant reveals how transformations scale space, while eigenvectors and eigenvalues highlight directions of invariant scaling.
*   These principles extend beyond simple geometric vectors to abstract spaces, including functions.